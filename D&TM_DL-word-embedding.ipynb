{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook created by: Gabriele Sottocornola\n",
    "for the M.Sc. class of Data & Text Mining\n",
    "UniversitÃ  degli studi di Milano-bicocca\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced python notebook that gives you an introduction to word embedding as a feature constructor for classification. The task consists on the sentiment classification of a set of movie reviews provided by users in IMDb. In order to do that we exploit the pre-trained 100d GloVe model (download it from the nlp.stanford website).\n",
    "\n",
    "After some pre-process we construct a padded (with the same SEQUENCE_LENGHT words) 3D matrix of word with their 100d representation in the GloVe embedding. \n",
    "Finally we can inject the created embedding lookup into a CNN (very similar to the MNIST one) and proceed to the training of the parameters for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "\n",
    "GLOVE_PATH = '.\\\\data\\\\glove.6B.100d.txt' # 100d GloVe embedding --> http://nlp.stanford.edu/data/glove.6B.zip\n",
    "DATA_PATH = '.\\\\data\\\\IMDb_reviews_sentiment_classification.csv'\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "SEQUENCE_LENGHT = 1000\n",
    "TEST_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "review_df = pd.read_csv(DATA_PATH, header=0, sep=',', encoding='latin')\n",
    "review_df = review_df.sample(frac=1) # shuffle data\n",
    "\n",
    "text_list = review_df['review'].tolist() # extract reviews into a list\n",
    "label_list = review_df['class'].tolist() # extract labels into a list\n",
    "label_list = [[0, 1] if el == 1 else [1, 0] for el in label_list] # labels one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123681 unique tokens\n",
      "The length of the longest sequence is 2442 tokens\n"
     ]
    }
   ],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "\n",
    "tokenizer = tf.contrib.keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS) # tokenizer: convert text to sequences of ids\n",
    "tokenizer.fit_on_texts(text_list) # fit list of text into tokenizer\n",
    "sequences = tokenizer.texts_to_sequences(text_list) \n",
    "\n",
    "word_index = tokenizer.word_index # complete dictionary\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "max_len = len(max(sequences, key=len)) # length of longest sequence\n",
    "print('The length of the longest sequence is {} tokens'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequence_list, max_seq_length):\n",
    "    \n",
    "    padded_seq = list()\n",
    "    for seq in sequence_list:\n",
    "        if len(seq) < max_seq_length:\n",
    "            seq = np.pad(seq, (0, max_seq_length - len(seq)), 'constant', constant_values=0) # pad with zeros\n",
    "        else:\n",
    "            seq = seq[:max_seq_length] # remove elements\n",
    "        padded_seq.append(seq)\n",
    "    \n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padded_seq = pad_sequences(sequences, SEQUENCE_LENGHT) # set sequences of tokens for each text to fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "# build index mapping words in the embeddings set to their embedding vector\n",
    "\n",
    "embeddings_index = {} # dictionary word:embedded vector\n",
    "with open(GLOVE_PATH, 'r', encoding='utf-8') as glove_f:    \n",
    "    \n",
    "    for line in glove_f:\n",
    "        values = line.split()\n",
    "        word = str(values[0])\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found {} word vectors of dimension {}'.format(len(embeddings_index), len(embeddings_index['the'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) # matrix of dimensions num_words (dictionary length), embedding length\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = None\n",
    "    if i < MAX_NB_WORDS:\n",
    "        # check if the word is one of the first MAX_NB_WORDS\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 48000\n",
      "Length of test data: 2000\n"
     ]
    }
   ],
   "source": [
    "train_seq = padded_seq[:-TEST_SIZE]\n",
    "train_labels = label_list[:-TEST_SIZE]\n",
    "test_seq = padded_seq[-TEST_SIZE:]\n",
    "test_labels = label_list[-TEST_SIZE:]\n",
    "\n",
    "print('Length of train data: {}'.format(len(train_seq)))\n",
    "print('Length of test data: {}'.format(len(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=1, padding='SAME')\n",
    "\n",
    "def max_pool1d(x, stride):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, stride, 1], strides=[1, 1, stride, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL CONSTRUCTION (COMPUTATION GRAPH)\n",
    "seq_input = tf.placeholder(tf.int64, [None, SEQUENCE_LENGHT])  #input text ids sequence\n",
    "emb_W = tf.Variable(tf.zeros(shape=[num_words, EMBEDDING_DIM]), trainable=False)\n",
    "emb_matrix = tf.placeholder(tf.float32, [num_words, EMBEDDING_DIM]) #inject embedding matrix into tf computational graph\n",
    "emb_init = emb_W.assign(emb_matrix)\n",
    "emb_layer = tf.nn.embedding_lookup(emb_W, seq_input) #lookup to the input sequence to the embedding vector space\n",
    "\n",
    "W_conv1 = weight_variable([5, 100, 128])\n",
    "b_conv1 = bias_variable([128])\n",
    "h_conv1 = tf.nn.relu(conv1d(emb_layer, W_conv1) + b_conv1)\n",
    "\n",
    "h_conv1 = tf.reshape(h_conv1, [-1, 1, 1000, 128])\n",
    "h_pool1 = max_pool1d(h_conv1, 5)\n",
    "h_pool1 = tf.reshape(h_pool1, [-1, 200, 128])\n",
    "\n",
    "W_conv2 = weight_variable([5, 128, 128])\n",
    "b_conv2 = bias_variable([128])\n",
    "h_conv2 = tf.nn.relu(conv1d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "h_conv2 = tf.reshape(h_conv2, [-1, 1, 200, 128])\n",
    "h_pool2 = max_pool1d(h_conv2, 5)\n",
    "h_pool2 = tf.reshape(h_pool2, [-1, 40, 128])\n",
    "\n",
    "W_conv3 = weight_variable([5, 128, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv1d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "h_conv3 = tf.reshape(h_conv3, [-1, 1, 40, 128])\n",
    "h_pool3 = max_pool1d(h_conv3, 5)\n",
    "\n",
    "W_fc1 = weight_variable([1024, 2])\n",
    "b_fc1 = bias_variable([2])\n",
    "\n",
    "h_flat = tf.reshape(h_pool3, [-1, 1024])\n",
    "pred_fc = tf.matmul(h_flat, W_fc1) + b_fc1 #predicted vector\n",
    "\n",
    "y_val = tf.placeholder(tf.int64, [None, 2]) #target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOSS FUNCTION AND TRAINING STEP\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_fc, labels=y_val))\n",
    "cross_entropy = cross_entropy + tf.nn.l2_loss(W_fc1) # cross entropy loss with l2 normalization\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# EVALUATION\n",
    "prediction = tf.argmax(pred_fc, 1) #predicted class\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(y_val, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training accuracy 0.578125\n",
      "test accuracy 0.5495\n",
      "step 200, training accuracy 0.617188\n",
      "test accuracy 0.5755\n",
      "step 300, training accuracy 0.626953\n",
      "test accuracy 0.6045\n",
      "step 400, training accuracy 0.65625\n",
      "test accuracy 0.6355\n",
      "step 500, training accuracy 0.708984\n",
      "test accuracy 0.669\n",
      "step 600, training accuracy 0.705078\n",
      "test accuracy 0.6815\n",
      "step 700, training accuracy 0.744141\n",
      "test accuracy 0.703\n",
      "step 800, training accuracy 0.753906\n",
      "test accuracy 0.7155\n",
      "step 900, training accuracy 0.761719\n",
      "test accuracy 0.7295\n",
      "step 1000, training accuracy 0.785156\n",
      "test accuracy 0.732\n",
      "step 1100, training accuracy 0.833984\n",
      "test accuracy 0.7445\n",
      "step 1200, training accuracy 0.818359\n",
      "test accuracy 0.749\n",
      "step 1300, training accuracy 0.826172\n",
      "test accuracy 0.7535\n",
      "step 1400, training accuracy 0.816406\n",
      "test accuracy 0.7615\n",
      "step 1500, training accuracy 0.806641\n",
      "test accuracy 0.7655\n",
      "step 1600, training accuracy 0.851562\n",
      "test accuracy 0.77\n",
      "step 1700, training accuracy 0.826172\n",
      "test accuracy 0.7725\n",
      "step 1800, training accuracy 0.845703\n",
      "test accuracy 0.767\n",
      "step 1900, training accuracy 0.828125\n",
      "test accuracy 0.775\n",
      "step 2000, training accuracy 0.849609\n",
      "test accuracy 0.7865\n",
      "step 2100, training accuracy 0.835938\n",
      "test accuracy 0.785\n",
      "step 2200, training accuracy 0.857422\n",
      "test accuracy 0.787\n",
      "step 2300, training accuracy 0.839844\n",
      "test accuracy 0.778\n",
      "step 2400, training accuracy 0.853516\n",
      "test accuracy 0.7875\n",
      "step 2500, training accuracy 0.833984\n",
      "test accuracy 0.782\n",
      "step 2600, training accuracy 0.859375\n",
      "test accuracy 0.7905\n",
      "step 2700, training accuracy 0.876953\n",
      "test accuracy 0.795\n",
      "step 2800, training accuracy 0.863281\n",
      "test accuracy 0.8025\n",
      "step 2900, training accuracy 0.867188\n",
      "test accuracy 0.803\n",
      "step 3000, training accuracy 0.867188\n",
      "test accuracy 0.8085\n",
      "step 3100, training accuracy 0.859375\n",
      "test accuracy 0.807\n",
      "step 3200, training accuracy 0.869141\n",
      "test accuracy 0.814\n",
      "step 3300, training accuracy 0.861328\n",
      "test accuracy 0.811\n",
      "step 3400, training accuracy 0.882812\n",
      "test accuracy 0.812\n",
      "step 3500, training accuracy 0.861328\n",
      "test accuracy 0.8165\n",
      "step 3600, training accuracy 0.855469\n",
      "test accuracy 0.815\n",
      "step 3700, training accuracy 0.835938\n",
      "test accuracy 0.803\n",
      "step 3800, training accuracy 0.878906\n",
      "test accuracy 0.824\n",
      "step 3900, training accuracy 0.875\n",
      "test accuracy 0.8265\n",
      "step 4000, training accuracy 0.882812\n",
      "test accuracy 0.822\n",
      "step 4100, training accuracy 0.878906\n",
      "test accuracy 0.8265\n",
      "step 4200, training accuracy 0.880859\n",
      "test accuracy 0.828\n",
      "step 4300, training accuracy 0.882812\n",
      "test accuracy 0.829\n",
      "step 4400, training accuracy 0.890625\n",
      "test accuracy 0.832\n",
      "step 4500, training accuracy 0.894531\n",
      "test accuracy 0.8325\n",
      "step 4600, training accuracy 0.888672\n",
      "test accuracy 0.8325\n",
      "step 4700, training accuracy 0.890625\n",
      "test accuracy 0.8275\n",
      "step 4800, training accuracy 0.878906\n",
      "test accuracy 0.8315\n",
      "step 4900, training accuracy 0.886719\n",
      "test accuracy 0.8365\n",
      "step 5000, training accuracy 0.876953\n",
      "test accuracy 0.821\n",
      "FINAL TEST ACCURACY 0.8335\n"
     ]
    }
   ],
   "source": [
    "# COMPUTATION PHASE\n",
    "num_iter = 5000\n",
    "batch_size = 512\n",
    "\n",
    "train_acc_list = list()\n",
    "test_acc_list = list()\n",
    "step_list = list()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(emb_init, feed_dict={emb_matrix: embedding_matrix})\n",
    "\n",
    "i = 1\n",
    "while i <= num_iter:\n",
    "    indeces = np.random.choice(np.arange(len(train_seq)), batch_size, replace=False)\n",
    "    batch_xs = [train_seq[idx] for idx in indeces]\n",
    "    batch_ys = [train_labels[idx] for idx in indeces]\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={seq_input: batch_xs, y_val: batch_ys, emb_matrix: embedding_matrix})\n",
    "        test_accuracy = accuracy.eval(feed_dict={seq_input: test_seq, y_val: test_labels, emb_matrix: embedding_matrix})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"test accuracy %g\"%(test_accuracy))\n",
    "    train_step.run(feed_dict={seq_input: batch_xs, y_val: batch_ys, emb_matrix: embedding_matrix})\n",
    "    i += 1\n",
    "\n",
    "print(\"FINAL TEST ACCURACY %g\"%accuracy.eval(feed_dict={seq_input: test_seq, y_val: test_labels, emb_matrix: embedding_matrix}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-aways\n",
    "\n",
    "+ Deep Learning can have excellent performance also on NLP tasks, i.e. sentiment analysis\n",
    "\n",
    "+ CNNs are exploited also in modeling text for their capabilities to find local patterns in data\n",
    "\n",
    "+ Dealing with text can be very tricky (due to the sparsity of data and the required computational power)\n",
    "\n",
    "+ Importing pre-trained model is often a good choice (e.g. word2vec, GloVe)\n",
    "\n",
    "+ Pre-trained models are also useful to initialize and fine tuning new embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
